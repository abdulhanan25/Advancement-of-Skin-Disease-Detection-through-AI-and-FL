{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IpjQ-UGqO02"
      },
      "source": [
        "# Download Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4eWBm-9qO05",
        "outputId": "b534243f-51b8-4961-a016-ae7f8dd57ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow opencv-python numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr_708VgqO08"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2ckSERGqO09",
        "outputId": "a4713354-2f01-4142-9e6a-4bda6ad5b9de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (2.13.0)\n",
            "Requirement already satisfied: opencv-python in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (1.24.3)\n",
            "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.24.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (67.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.58.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.29.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: urllib3<2.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.5.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\fatima azfar\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VeTtvJ7qO0-"
      },
      "source": [
        "# U-Net Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8JXGLfvqO0_",
        "outputId": "76117308-57a7-4826-eaa5-3bab49134f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46/46 [==============================] - 405s 9s/step\n"
          ]
        }
      ],
      "source": [
        "def create_unet_model(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    # Add more convolution and pooling layers here as needed\n",
        "    # Example:\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    # ...\n",
        "\n",
        "    # Decoder\n",
        "    up3 = UpSampling2D(size=(2, 2))(conv3)  # Ensure you define 'conv3' here\n",
        "    concat3 = concatenate([conv2, up3], axis=-1)\n",
        "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(concat3)\n",
        "\n",
        "    # Add more upsampling and concatenation layers as needed\n",
        "    # Example:\n",
        "    up4 = UpSampling2D(size=(2, 2))(conv4)\n",
        "    concat4 = concatenate([conv1, up4], axis=-1)\n",
        "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(concat4)\n",
        "\n",
        "    # ...\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(conv5)  # Ensure you define 'conv5' here\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (256, 256, 3)  # Adjust the input shape according to your images\n",
        "\n",
        "# Create the U-Net model\n",
        "model = create_unet_model(input_shape)\n",
        "\n",
        "# Define the path to your image directory\n",
        "image_dir = r'D:\\Users\\Nadeem\\Desktop\\BSDS\\FYP\\Data\\Acne\\Classification\\JPEGImages'\n",
        "\n",
        "# Load and preprocess images from the directory\n",
        "def load_and_preprocess_images(image_dir, input_shape):\n",
        "    image_paths = glob.glob(os.path.join(image_dir, '*.jpg'))\n",
        "    images = []\n",
        "    for image_path in image_paths:\n",
        "        img = cv2.imread(image_path)\n",
        "        img = cv2.resize(img, (input_shape[0], input_shape[1]))\n",
        "        img = img / 255.0  # Normalize to [0, 1]\n",
        "        images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Load the images\n",
        "images = load_and_preprocess_images(image_dir, input_shape)\n",
        "\n",
        "# Predict the segmentation masks for the loaded images\n",
        "masks = model.predict(images)\n",
        "\n",
        "# Now you have the segmentation masks for the images in the 'masks' variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JVa1A_6qO1A",
        "outputId": "976f8881-fe63-4a40-b0e6-8ba6d77abcea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 332ms/step\n"
          ]
        }
      ],
      "source": [
        "new_image_path = 'acne-cystic-46.jpg'\n",
        "new_image = cv2.imread(new_image_path)\n",
        "input_shape = (256, 256)  # Specify the desired input shape\n",
        "new_image = cv2.resize(new_image, (input_shape[0], input_shape[1]))\n",
        "new_image_normalized = new_image / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "# Predict the segmentation mask for the new image\n",
        "segmentation_mask = model.predict(np.expand_dims(new_image_normalized, axis=0))[0]\n",
        "\n",
        "# Threshold the mask to get binary segmentation\n",
        "threshold = 0.5  # Adjust as needed\n",
        "binary_mask = (segmentation_mask > threshold).astype(np.uint8)\n",
        "\n",
        "# Create a green mask\n",
        "green_mask = np.zeros_like(new_image)\n",
        "green_mask[:,:,1] = 255  # Set the green channel to 255 (green color)\n",
        "\n",
        "# Apply the binary mask to the green mask\n",
        "segmented_image = cv2.bitwise_and(green_mask, green_mask, mask=binary_mask)\n",
        "\n",
        "# Overlay the segmented mask on the original image\n",
        "overlay = cv2.addWeighted(new_image, 0.7, segmented_image, 0.3, 0)\n",
        "\n",
        "# Display the original image and segmented result side by side\n",
        "cv2.imshow('Original Image', new_image)\n",
        "cv2.imshow('Segmented Image', overlay)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7TsXC0FqO1B"
      },
      "source": [
        "# Seg-Net Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPujwNQtqO1B",
        "outputId": "45bad7bf-f09c-4156-d3ff-35573a292134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 256, 256, 64)      1792      \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 256, 256, 64)      256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 256, 256, 64)      36928     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 256, 256, 64)      256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 128, 128, 64)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 128, 128, 128)     73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 128, 128, 128)     512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 128, 128, 128)     147584    \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 128, 128, 128)     512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 64, 64, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " up_sampling2d (UpSampling2  (None, 128, 128, 128)     0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 128, 128, 128)     147584    \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 128, 128, 128)     512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 128, 128, 128)     147584    \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 128, 128, 128)     512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " up_sampling2d_1 (UpSamplin  (None, 256, 256, 128)     0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 256, 256, 64)      73792     \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 256, 256, 64)      256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 256, 256, 64)      36928     \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 256, 256, 64)      256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 256, 256, 21)      1365      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 670485 (2.56 MB)\n",
            "Trainable params: 668949 (2.55 MB)\n",
            "Non-trainable params: 1536 (6.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_segnet_model(input_shape, num_classes):\n",
        "    # Define the input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "\n",
        "    # Decoder\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = UpSampling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Conv2D(num_classes, (1, 1), activation='softmax', padding='valid')(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "input_shape = (256, 256, 3)  # Adjust the input shape according to your images\n",
        "num_classes = 21  # Number of segmentation classes (adjust as needed)\n",
        "\n",
        "segnet_model = create_segnet_model(input_shape, num_classes)\n",
        "\n",
        "segnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "segnet_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0eadgnCqO1C",
        "outputId": "eb51ebf0-26b6-4f47-98f5-ddc88c51d6a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 273ms/step\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the image\n",
        "image_path = 'acne-cystic-46.jpg'\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.resize(image, (256, 256))  # Resize to match the input size of the model\n",
        "image_normalized = image / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "# Perform inference to obtain the segmentation mask\n",
        "segmentation_mask = segnet_model.predict(np.expand_dims(image_normalized, axis=0))[0]\n",
        "\n",
        "# Convert the mask to a colored segmentation map\n",
        "colored_mask = np.zeros((256, 256, 3), dtype=np.uint8)\n",
        "\n",
        "# Define colors for each class (adjust as needed)\n",
        "class_colors = {\n",
        "    0: [0, 0, 0],    # Background (black)\n",
        "    1: [0, 255, 0],  # Class 1 (green)\n",
        "    2: [0, 0, 255],  # Class 2 (red)\n",
        "    # Add more colors for additional classes as necessary\n",
        "}\n",
        "\n",
        "# Assign colors to each pixel based on the class with the highest probability\n",
        "for class_index, color in class_colors.items():\n",
        "    mask = segmentation_mask[:, :, class_index]\n",
        "    colored_mask[mask > 0.5] = color\n",
        "\n",
        "# Overlay the colored mask on the original image\n",
        "segmented_image = cv2.addWeighted(image, 0.7, colored_mask, 0.3, 0)\n",
        "\n",
        "# Display the original image and segmented result\n",
        "cv2.imshow('Original Image', image)\n",
        "cv2.imshow('Segmented Image', segmented_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BU5Z68UqO1F"
      },
      "source": [
        "# Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy8OR9-HqO1F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nHsS1I4qO1G"
      },
      "outputs": [],
      "source": [
        "data_dir = 'D:\\\\Users\\\\Data\\\\Main_data'\n",
        "image_size = (224, 224)\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5OY_q8JqO1G",
        "outputId": "92fbe7ab-f5b0-481d-823a-5e48c7d88780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5946 images belonging to 9 classes.\n",
            "Found 1482 images belonging to 9 classes.\n"
          ]
        }
      ],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2  # Split 20% of the data for validation\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIgZwN3xqO1H",
        "outputId": "700a4952-dd70-4e63-c915-4d07635124aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 6s 1us/step\n"
          ]
        }
      ],
      "source": [
        "base_model = MobileNetV2(input_shape=(image_size[0], image_size[1], 3), include_top=False, weights='imagenet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoFxWYIkqO1H"
      },
      "outputs": [],
      "source": [
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(9, activation='softmax')(x)  # 9 classes for your skin diseases\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he7s3ZYCqO1I",
        "outputId": "47fef5c7-9564-46f8-91e4-9fadacc816f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TedjggZWqO1I",
        "outputId": "720e4baf-12ee-4070-be81-cd851e69d5d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "185/185 [==============================] - 1057s 6s/step - loss: 1.0278 - accuracy: 0.5960 - val_loss: 15.1489 - val_accuracy: 0.0890\n",
            "Epoch 2/20\n",
            "185/185 [==============================] - 908s 5s/step - loss: 0.8921 - accuracy: 0.6561 - val_loss: 5.7942 - val_accuracy: 0.2887\n",
            "Epoch 3/20\n",
            "185/185 [==============================] - 924s 5s/step - loss: 0.8043 - accuracy: 0.6923 - val_loss: 17.5522 - val_accuracy: 0.1827\n",
            "Epoch 4/20\n",
            "185/185 [==============================] - 918s 5s/step - loss: 0.7134 - accuracy: 0.7208 - val_loss: 10.0157 - val_accuracy: 0.2276\n",
            "Epoch 5/20\n",
            "185/185 [==============================] - 933s 5s/step - loss: 0.7165 - accuracy: 0.7274 - val_loss: 17.9518 - val_accuracy: 0.2276\n",
            "Epoch 6/20\n",
            "185/185 [==============================] - 924s 5s/step - loss: 0.6628 - accuracy: 0.7454 - val_loss: 4.6493 - val_accuracy: 0.2908\n",
            "Epoch 7/20\n",
            "185/185 [==============================] - 864s 5s/step - loss: 0.6137 - accuracy: 0.7587 - val_loss: 4.1836 - val_accuracy: 0.3152\n",
            "Epoch 8/20\n",
            "185/185 [==============================] - 878s 5s/step - loss: 0.6114 - accuracy: 0.7653 - val_loss: 4.0340 - val_accuracy: 0.3614\n",
            "Epoch 9/20\n",
            "185/185 [==============================] - 854s 5s/step - loss: 0.5655 - accuracy: 0.7825 - val_loss: 5.3010 - val_accuracy: 0.3580\n",
            "Epoch 10/20\n",
            "185/185 [==============================] - 823s 4s/step - loss: 0.5624 - accuracy: 0.7885 - val_loss: 10.3825 - val_accuracy: 0.2874\n",
            "Epoch 11/20\n",
            "185/185 [==============================] - 852s 5s/step - loss: 0.5604 - accuracy: 0.7888 - val_loss: 4.0598 - val_accuracy: 0.4721\n",
            "Epoch 12/20\n",
            "185/185 [==============================] - 825s 4s/step - loss: 0.5695 - accuracy: 0.7885 - val_loss: 7.5704 - val_accuracy: 0.2337\n",
            "Epoch 13/20\n",
            "185/185 [==============================] - 830s 4s/step - loss: 0.5853 - accuracy: 0.7793 - val_loss: 7.3327 - val_accuracy: 0.3845\n",
            "Epoch 14/20\n",
            "185/185 [==============================] - 831s 4s/step - loss: 0.5195 - accuracy: 0.7981 - val_loss: 4.4776 - val_accuracy: 0.3336\n",
            "Epoch 15/20\n",
            "185/185 [==============================] - 825s 4s/step - loss: 0.5002 - accuracy: 0.8091 - val_loss: 4.3188 - val_accuracy: 0.3872\n",
            "Epoch 16/20\n",
            "185/185 [==============================] - 824s 4s/step - loss: 0.4738 - accuracy: 0.8150 - val_loss: 5.9872 - val_accuracy: 0.3091\n",
            "Epoch 17/20\n",
            "185/185 [==============================] - 828s 4s/step - loss: 0.4435 - accuracy: 0.8345 - val_loss: 5.4154 - val_accuracy: 0.2982\n",
            "Epoch 18/20\n",
            "185/185 [==============================] - 837s 5s/step - loss: 0.4417 - accuracy: 0.8318 - val_loss: 4.9898 - val_accuracy: 0.3852\n",
            "Epoch 19/20\n",
            "185/185 [==============================] - 836s 5s/step - loss: 0.4766 - accuracy: 0.8250 - val_loss: 6.1679 - val_accuracy: 0.3064\n",
            "Epoch 20/20\n",
            "185/185 [==============================] - 828s 4s/step - loss: 0.4489 - accuracy: 0.8382 - val_loss: 6.5519 - val_accuracy: 0.2874\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfIredMDqO1J",
        "outputId": "99f63a0d-db81-4381-e928-850b5ac07d49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47/47 [==============================] - 72s 2s/step - loss: 6.6139 - accuracy: 0.2901\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[6.613879680633545, 0.29014843702316284]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(validation_generator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXgCywJ-qO1J"
      },
      "source": [
        "# Testing of Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rdzIat9qO1J"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.load_model('skin_disease_classification_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHss7FZ9qO1K",
        "outputId": "c41f7d3c-386e-4887-d3fe-0da3a1303b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 83ms/step\n",
            "Predicted class: Urticaria Hives\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "new_image = cv2.imread('monkeypox1.png')\n",
        "new_image = cv2.cvtColor(new_image, cv2.COLOR_BGR2RGB)\n",
        "new_image = cv2.resize(new_image, (224, 224))\n",
        "new_image = np.expand_dims(new_image, axis=0) / 255.0  # Normalize the image\n",
        "\n",
        "predictions = model.predict(new_image)\n",
        "\n",
        "class_labels = ['Acne', 'Atopic Dermatitis', 'Chickenpox', 'Eczema', 'Measles', 'Monkeypox', 'Normal', 'Psoriais', 'Urticaria Hives']  # Replace with your class labels\n",
        "predicted_class_index = np.argmax(predictions)\n",
        "predicted_class = class_labels[predicted_class_index]\n",
        "\n",
        "print(f'Predicted class: {predicted_class}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}